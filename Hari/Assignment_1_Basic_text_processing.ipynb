{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1: Basic text processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0rrT3WrzYbj"
      },
      "source": [
        "**Assignment 1: Basic text processing**\n",
        "\n",
        "**Author- Hariharan Gopinath, Sanjeev madhavan, Colton Cunov**\n",
        "\n",
        "**DAT450-Machine learning for natural language processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqu-y2eh0a4Q"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSJBUHAUzxbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c278b90-0c7f-485f-ecbb-dccca01a25f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyuV104C1-fA",
        "outputId": "dfb88ee1-06c9-4aa3-995b-fe95f68881bf"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP course/Assignment-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP course/Assignment-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly6zagzl2XhF",
        "outputId": "348f3d92-881e-4f4c-faa6-bf54640d4f9e"
      },
      "source": [
        " #!unzip '/content/drive/MyDrive/NLP course/Assignment-1/a1_data.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/NLP course/Assignment-1/a1_data.zip\n",
            "replace a1_data/europarl.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOxbWFo5TJ1"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI4NGuIi7vyN"
      },
      "source": [
        "import spacy\n",
        "from collections import Counter, defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZK5tbj84pE"
      },
      "source": [
        "### Warmup: computing word frequencies\n",
        "#### most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0dRCuQP8_Fr"
      },
      "source": [
        "file_names = ['books.txt','europarl.txt','wikipedia.txt']\n",
        "encodings = ['ISO-8859-1','utf-8','utf-8']\n",
        "base_dir = 'C:/Users/Colton/OneDrive/School/Machine Learning for Natural Language Processing/HW_1/a1_data/' # fix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jagIDL_o0iN_"
      },
      "source": [
        "def get_frequency_counter(file_name,encoding):\n",
        "    freqs = Counter()\n",
        "    with open(base_dir + file_name, encoding = encoding) as f:\n",
        "        for line in f:\n",
        "            tokens = line.lower().split()\n",
        "            for token in tokens:\n",
        "                freqs[token] += 1\n",
        "    return freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMlfqpmz9RDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "bd53469f-f263-4f72-a2f4-206351461c6a"
      },
      "source": [
        "freqs = {}\n",
        "for i in range(len(file_names)):\n",
        "    file_name = file_names[i]\n",
        "    encoding = encodings[i]\n",
        "    freqs[file_name] = get_frequency_counter(file_name, encoding)\n",
        "    print('--------------' + file_name + '-------------')\n",
        "    for word, freq in freqs[file_name].most_common(10):\n",
        "        print(word + '\\t' + str(freq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-73602b39015e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_frequency_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c8679c784b73>\u001b[0m in \u001b[0;36mget_frequency_counter\u001b[0;34m(file_name, encoding)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_frequency_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwFOs91Y9XEx"
      },
      "source": [
        "#### dictionary-within-dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVjxjGWs9WU7"
      },
      "source": [
        "print('Most common words following \"red\"')\n",
        "for i in range(len(file_names)):\n",
        "    file_name = file_names[i]\n",
        "    encoding = encodings[i]\n",
        "    freqs = defaultdict(Counter)\n",
        "    with open(base_dir+file_name, encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            tokens = line.lower().split()\n",
        "            for t1, t2 in zip(tokens, tokens[1:]):\n",
        "                freqs[t1][t2] += 1\n",
        "    word = freqs['red'].most_common(1)[0][0]\n",
        "    freq = freqs['red'].most_common(1)[0][1]\n",
        "    print(file_name + ':\\t' + word + '\\t' + str(freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCzebd8XC5iP"
      },
      "source": [
        "### Investigating the word frequency distribution\n",
        "Our reflection goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZHHCB-vC6Bj"
      },
      "source": [
        "# make sure freqs is list of get_frequency_counter()\n",
        "#  i.e. \n",
        "#   freqs = {}\n",
        "#   for i in range(len(file_names)):\n",
        "#     freqs[i] = get_frequency_counter(file_names[i], encodings[i])\n",
        "word_freqs = []\n",
        "plt.subplot(121)\n",
        "for i in range(len(file_names)):\n",
        "    file_name = file_names[i]\n",
        "    word_freqs.append([x[1] for x in freqs[file_name].most_common(100)])\n",
        "    plt.plot(word_freqs[i],label=file_name[:-4])\n",
        "plt.legend()\n",
        "plt.title('Linear')\n",
        "\n",
        "plt.subplot(122)\n",
        "for i in range(len(word_freqs)):\n",
        "    plt.loglog(word_freqs[i],label=file_names[i][:-4])\n",
        "plt.legend()\n",
        "plt.title('Log-Log')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I8Mk96pDojX"
      },
      "source": [
        "### Comparing corpora\n",
        "*Is this asking us to find \"words from corpora[i] that are common in corpora[j]\"?*\n",
        "*or maybe he wants a heatmap of frequency?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQhOzkUFDxB4"
      },
      "source": [
        "# freqs = {}\n",
        "# for i in range(len(file_names)):\n",
        "#     freqs[i] = get_frequency_counter(file_names[i], encodings[i])\n",
        "for i in range(len(file_names)):\n",
        "    file_name_1 = file_names[i]\n",
        "    for j in range(len(file_names)):\n",
        "        file_name_2 = file_names[j]\n",
        "        if file_name_1 == file_name_2:\n",
        "            continue\n",
        "        print('\\n')\n",
        "        print('Top 10 words from \"' + file_name_1 + '\" (first) and their relative frequency in \"' + file_name_2 + '\" (second):')\n",
        "        print('WORD\\tFREQ\\tFREQ')\n",
        "        print('--------------------')\n",
        "        top_10_counter = freqs[file_name_1].most_common(10)\n",
        "        top_10_words = [x[0] for x in top_10_counter]\n",
        "        relative_freq_1 = [x[1] / sum(freqs[file_name_1].values()) for x in top_10_counter]\n",
        "        relative_freq_2 = []\n",
        "        for word in top_10_words:\n",
        "            relative_freq_2.append(freqs[file_name_2][word] / sum(freqs[file_name_2].values()))\n",
        "        for i in range(len(top_10_words)):\n",
        "            print(top_10_words[i] + '\\t' + str(round(100*relative_freq_1[i],2)) + '%\\t' + str(round(100*relative_freq_2[i],2)) + '%')\n",
        "                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAIrAsnMfHzp"
      },
      "source": [
        "### Side show: preprocessing text for machine learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlN029NBfKRz"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, dataset, max_voc_size=1000, batch_size=1000):\n",
        "        ############# word-to-int mapping #############\n",
        "        self.dataset = dataset\n",
        "        self.max_voc_size = max_voc_size\n",
        "        freqs = Counter()\n",
        "        tokens = dataset.lower().split()\n",
        "        for token in tokens:\n",
        "            freqs[token] += 1\n",
        "        # freqs is dict(), i.e. freqs[word] = count\n",
        "        if len(freqs) > max_voc_size:\n",
        "            voc_size = max_voc_size\n",
        "        else:\n",
        "            voc_size = len(freqs)\n",
        "        freqs = freqs.most_common(voc_size)\n",
        "        # now freqs is tuple of (word, count)\n",
        "        mapping = {freqs[i][0]:i+1 for i in range(voc_size)}\n",
        "        # now freqs is dict of freqs[word]=r where r=rank based on frequency, r=(1,...)\n",
        "        self.mapping = mapping # should use self.mapping.get('key',-1) to try non-existent keys\n",
        "\n",
        "        \n",
        "        ############# batching #####################\n",
        "        split_dataset = dataset.split('\\n')\n",
        "        if len(split_dataset) > batch_size:\n",
        "            split_dataset = split_dataset[:batch_size]\n",
        "        else:\n",
        "            batch_size = len(split_dataset)\n",
        "\n",
        "        max_line_length = len(max(split_dataset, key=len))\n",
        "        batches = torch.ones(max_line_length, batch_size) * -1 # -1 is the padding\n",
        "        for line_idx in range(len(split_dataset)):\n",
        "            line = split_dataset[line_idx]\n",
        "            split_line = line.split()\n",
        "            for word_idx in range(len(split_line)):\n",
        "                batches[word_idx][line_idx] = mapping.get(split_line[word_idx], 0) # 0 is what words not in the mapping map to\n",
        "        self.batches = batches\n",
        "            \n",
        "        \n",
        "i = 0\n",
        "max_voc_size=1000\n",
        "batch_size=1000\n",
        "with open(base_dir+file_names[i], encoding=encodings[i]) as f:\n",
        "    dataset = f.read()\n",
        "    \n",
        "vc = Vocab(dataset, max_voc_size)\n",
        "print(vc.mapping)\n",
        "print(vc.batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0I-lY8EfLkh"
      },
      "source": [
        "### Trying out an NLP toolkit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjK0Dg4kfM9j"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# !python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxAhb7TkfN9j"
      },
      "source": [
        "#### Processing a text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrErQtXefPFo"
      },
      "source": [
        "example = 'Apple bought two companies this year and no one knew, Mark Gurman at 9to5Mac reports.'\n",
        "result = nlp(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bcJOTUofRTb"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(result, style='ent', jupyter=True)\n",
        "# displacy.render(result, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-qNJNlKfRuW"
      },
      "source": [
        "for token in result:\n",
        "    print('Text: ' + token.text + \n",
        "          '\\tPoS: ' + token.pos_ + \n",
        "          '\\tLemma: ' + token.lemma_ + \n",
        "          '\\tHead: ' + str(token.head) + \n",
        "          '\\tDep: ' + str(token.dep_))\n",
        "    \n",
        "print('----------')\n",
        "for entity in result.ents:\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyh659ibfUDL"
      },
      "source": [
        "Obviously, \"two\" is not a named entity. It got it right that \"one\" in this case is a noun, not a number while \"two\" is a number. However \"9to5Mac\" is not a number. Also, \"reports\" is a verb not a noun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhaFSTyCfXTt"
      },
      "source": [
        "### Additional questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSeDCERvtsG8"
      },
      "source": [
        "corpora = {}\n",
        "for i in range(len(file_names)):\n",
        "    with open(base_dir+file_names[i],encoding=encodings[i]) as f:\n",
        "        corpora[file_names[i]] = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov5giuyHtpUZ"
      },
      "source": [
        "#### Which are the most frequent nouns in the book review corpus?\n",
        "SpaCy tags entities as GPE (GeoPolitical Entity?) if they are countries, states, cities, provinces, etc. We could use geonamescache to validate if a token is actually a country, but I don't think that's what they really want us to do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBmBv_8UfSI6"
      },
      "source": [
        "num_lines = 1000\n",
        "nouns = Counter()\n",
        "for line in corpora['books.txt'].split('\\n')[:num_lines]:\n",
        "    result = nlp(line)\n",
        "    for token in result:\n",
        "        if token.pos_ == 'NOUN':\n",
        "            nouns[token.text] += 1\n",
        "print(nouns.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc08PG4Nt6uk"
      },
      "source": [
        "#### Which are the most frequently mentioned countries in the Wikipedia corpus?\n",
        "SpaCy tags entities as GPE (GeoPolitical Entity?) if they are countries, states, cities, provinces, etc.\n",
        "We could use *geonamescache* to validate if a token is actually a country, but I don't think that's what they really want us to do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJeHvd_Ft8nf"
      },
      "source": [
        "num_lines = 1000\n",
        "countries = Counter()\n",
        "for line in corpora['wikipedia.txt'].split('\\n')[:num_lines]:\n",
        "    result = nlp(line)\n",
        "    for entity in result.ents:\n",
        "        if entity.label_ == 'GPE':\n",
        "            countries[entity.text] += 1\n",
        "print(countries.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jhIAq2ptzrf"
      },
      "source": [
        "#### What are the items that people drink most frequently in the European Parliament corpus?\n",
        "To count this, we look at words with POS=noun, HEAD = inflection of *drink*, and DEPENDENCY = \"dobj\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ektEvyDt0v6"
      },
      "source": [
        "inflected_forms = ['drink','drank','drunk','drinking','drinks','Drink','Drank','Drunk','Drinking','Drinks']\n",
        "drinks = Counter()\n",
        "for line in corpora['europarl.txt'].split('\\n'):\n",
        "    for inflected_form in inflected_forms:\n",
        "        if inflected_form in line:\n",
        "            result = nlp(line)\n",
        "            for token in result:\n",
        "                if token.pos_=='NOUN' and \\\n",
        "                    str(token.head) == inflected_form and \\\n",
        "                    token.dep_ == 'dobj' and \\\n",
        "                    token.text != inflected_form: #idk if this last one is necessary\n",
        "                    drinks[token.text] += 1\n",
        "print(drinks.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}